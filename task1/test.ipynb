{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7f4f4a",
   "metadata": {},
   "source": [
    "## A simple notebook to test some functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afe7e5",
   "metadata": {},
   "source": [
    "#### Activation functions: sigmoid and ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459e72b",
   "metadata": {},
   "source": [
    "Sigmoid and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "fedd5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999997739675702 2.260319188887599e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    s = 1/(1 + np.exp(-x))\n",
    "\n",
    "    return s\n",
    "\n",
    "def derivative_sigmoid(s):\n",
    "\n",
    "    ds = s*(1-s)\n",
    "\n",
    "    return ds\n",
    "\n",
    "w = [1,2]\n",
    "x = [2,4]\n",
    "b = 3\n",
    "z = np.dot(w,x) + b\n",
    "a = sigmoid(z)\n",
    "da = derivative_sigmoid(a)\n",
    "print(a,da)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ef17c",
   "metadata": {},
   "source": [
    "ReLU and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "10c95356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return 1 * (x>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c803de9",
   "metadata": {},
   "source": [
    "Now let's implement the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "24737ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "bb9d9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_softmax(vector):\n",
    "    s = softmax(vector)\n",
    "    s = s.reshape(-1, 1)\n",
    "\n",
    "    return np.diagflat(s) - np.dot(s, s.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120e733",
   "metadata": {},
   "source": [
    "### Implement a fully parametrizable neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "45cc2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     species  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "145        2  \n",
       "146        2  \n",
       "147        2  \n",
       "148        2  \n",
       "149        2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['species'] = iris.target\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7fcf7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1a01f",
   "metadata": {},
   "source": [
    "## Create a NN class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde4cfa",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4a1a8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "# just 4 samples\n",
    "X = np.array(X_train)\n",
    "\n",
    "# target values \n",
    "y = np.array(y_train).T \n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    #hidden layer (sigmoid, relu)\n",
    "    #output layer (sigmoid, softmax)\n",
    "\n",
    "    def __init__(self, activation_function, no_of_input_nodes, no_of_hidden_nodes, no_of_output_nodes, n_epochs, activation_output):\n",
    "        self.hidden_layers = len(no_of_hidden_nodes)\n",
    "        self.activation_function = activation_function\n",
    "        self.no_of_input_nodes = no_of_input_nodes # as many as the dataset's features\n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes # no fixed number, needs tuning\n",
    "        self.no_of_output_nodes = no_of_output_nodes # as many as the output classes\n",
    "        self.n_epochs = n_epochs\n",
    "        self.activation_output = activation_output #activation function of output layer (softmax/sigmoid)\n",
    "\n",
    "        self.weights, self.biases = self.weights_and_bias()\n",
    "\n",
    "    def weights_and_bias(self):\n",
    "        layers = [self.no_of_input_nodes] + self.no_of_hidden_nodes + [self.no_of_output_nodes] #e.g. [2,3,5,2]\n",
    "        weights = [] #TODO: weight and bias proper initialization (xavier)\n",
    "        biases = []\n",
    "        for i in range(len((layers))-1):\n",
    "\n",
    "            n_in = layers[i]\n",
    "            n_out = layers[i+1]\n",
    "\n",
    "            weights.append(2*np.random.random((n_in,n_out)) - 1)\n",
    "            biases.append(np.zeros((1, n_out)))\n",
    "        return weights, biases\n",
    "\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "\n",
    "        #first hidden layer needs to have the actual data\n",
    "        #all other hidden layers take the result from the previous layer\n",
    "        a = X\n",
    "        for layer in range(self.hidden_layers):\n",
    "\n",
    "            W = self.weights[layer]\n",
    "            b = self.biases[layer]\n",
    "            z = np.dot(a, W) + b\n",
    "\n",
    "            if self.activation_function =='sigmoid':\n",
    "                a = sigmoid(z)\n",
    "            elif self.activation_function =='relu':\n",
    "                a = relu(z)\n",
    "            print(f'Shape of layer: {a.shape}')\n",
    "\n",
    "        #---Output Layer---\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        z = np.dot(a, W) + b\n",
    "        if self.activation_output == 'sigmoid':\n",
    "            a = sigmoid(z)\n",
    "        elif self.activation_output == 'softmax':\n",
    "            a = softmax(z)\n",
    "        print(f'Shape of output layer: {a.shape}')\n",
    "        return a\n",
    "        #TODO: compute loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "186ee072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of layer: (120, 3)\n",
      "Shape of layer: (120, 4)\n",
      "Shape of output layer: (120, 3)\n"
     ]
    }
   ],
   "source": [
    "#TODO: run the forward pass in epochs\n",
    "#n_epochs = 500\n",
    "#for epoch in range(n_epochs):\n",
    "test = NeuralNetwork('sigmoid',4,[3,4],3,500,'softmax')\n",
    "output = test.forward_pass(X_train)\n",
    "\n",
    "#backward_pass_hidden_layer(self)\n",
    "#backward_pass_output_layer(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "abc9fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of layer: (120, 3)\n",
      "Shape of layer: (120, 4)\n",
      "Shape of output layer: (120, 3)\n",
      "(120, 3)\n"
     ]
    }
   ],
   "source": [
    "y_pred = test.forward_pass(X_train)\n",
    "print(y_pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "7fc12d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008592303590349501 0.005517008648874792\n",
      "[0.00844186 0.00846027 0.00833157 0.00842607 0.00841794]\n"
     ]
    }
   ],
   "source": [
    "print(np.min(y_pred), np.max(y_pred))\n",
    "print(np.sum(y_pred, axis=1)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9db9d-684a-4672-ae6a-66bb1dd8e9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929c85b-019b-41a0-9df3-01dcad5bf29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df1456-c064-4111-a472-4414bd849757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70023f8d-5132-4737-8972-08ae3cde0a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1d149-1896-4e94-aed5-d07434fae875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231b19b-d893-446f-a47f-64137dc93a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1278ce5-2522-4c00-a915-428d0226ab6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00e5f6-cfd7-49d2-a0cd-4f87e6f9ac9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "51c0f19b-b0af-45a3-83a0-202b011248c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    # z: (N, num_classes)\n",
    "    shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "    e = np.exp(shifted)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    #hidden layer (sigmoid, relu)\n",
    "    #output layer (sigmoid, softmax)\n",
    "\n",
    "    def __init__(self, activation_function, no_of_input_nodes,\n",
    "                 no_of_hidden_nodes, no_of_output_nodes,\n",
    "                 n_epochs, activation_output, learning_rate=0.01):    # CHANGED learning_rate=0.01\n",
    "        self.hidden_layers = len(no_of_hidden_nodes)\n",
    "        self.activation_function = activation_function\n",
    "        self.no_of_input_nodes = no_of_input_nodes\n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
    "        self.no_of_output_nodes = no_of_output_nodes\n",
    "        self.n_epochs = n_epochs\n",
    "        self.activation_output = activation_output\n",
    "        self.learning_rate = learning_rate     # CHANGED\n",
    "\n",
    "        self.weights, self.biases = self.weights_and_bias()\n",
    "\n",
    "    def weights_and_bias(self):\n",
    "        layers = [self.no_of_input_nodes] + self.no_of_hidden_nodes + [self.no_of_output_nodes]\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            n_in = layers[i]\n",
    "            n_out = layers[i+1]\n",
    "            W = 2*np.random.random((n_in, n_out)) - 1\n",
    "            b = np.zeros((1, n_out))\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "\n",
    "\n",
    "        # create empty lists to store cache for backward\n",
    "        self.activations = []          # CHANGED\n",
    "        self.zs = []                   # CHANGED\n",
    "\n",
    "\n",
    "        \n",
    "        a = X\n",
    "        self.activations.append(a)     # CHANGED: store a0\n",
    "\n",
    "        \n",
    "        for layer in range(self.hidden_layers):\n",
    "            W = self.weights[layer]\n",
    "            b = self.biases[layer]\n",
    "            z = np.dot(a, W) + b\n",
    "\n",
    "            self.zs.append(z)          # CHANGED: store z^{l+1}\n",
    "\n",
    "            if self.activation_function == 'sigmoid':\n",
    "                a = sigmoid(z)\n",
    "            elif self.activation_function == 'relu':\n",
    "                a = relu(z)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported hidden activation\")\n",
    "\n",
    "            self.activations.append(a) # CHANGED: store a^{l+1}\n",
    "\n",
    "        \n",
    "        # output layer\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        z = np.dot(a, W) + b\n",
    "\n",
    "        self.zs.append(z)              # CHANGED: store z^L\n",
    "\n",
    "        \n",
    "        if self.activation_output == 'sigmoid':\n",
    "            a = sigmoid(z)\n",
    "        elif self.activation_output == 'softmax':\n",
    "            a = softmax(z)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported output activation\")\n",
    "\n",
    "        self.activations.append(a)     # CHANGED: store a^L\n",
    "        \n",
    "        return a\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "    \n",
    "        # number of samples\n",
    "        N = y_true.shape[0]\n",
    "        \n",
    "        correct_probs = y_pred[np.arange(N), y_true]\n",
    "    \n",
    "        # loss = average of -log(p)\n",
    "        loss = -np.sum(np.log(correct_probs)) / N\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, y_pred):\n",
    "\n",
    "        \n",
    "        N = X.shape[0]   # number of samples\n",
    "    \n",
    "        # make empty lists for gradients\n",
    "        dW = [None] * len(self.weights)\n",
    "        db = [None] * len(self.biases)\n",
    "    \n",
    "        # output layer gradient\n",
    "        \n",
    "        y_true = y_true.astype(int)\n",
    "    \n",
    "        # dZ for softmax + cross entropy\n",
    "        dZ = y_pred.copy()\n",
    "        dZ[np.arange(N), y_true] -= 1\n",
    "        dZ = dZ / N\n",
    "    \n",
    "        # index of last layer\n",
    "        L = len(self.weights) - 1\n",
    "    \n",
    "        # gradients for last layer\n",
    "        # activations[L] = a^{L-1}\n",
    "        dW[L] = np.dot(self.activations[L].T, dZ)\n",
    "        db[L] = np.sum(dZ, axis=0, keepdims=True)\n",
    "    \n",
    "        # dA for the layer below\n",
    "        dA = np.dot(dZ, self.weights[L].T)\n",
    "    \n",
    "        # hidden layers (from last to first)\n",
    "        \n",
    "        for l in range(self.hidden_layers - 1, -1, -1):\n",
    "    \n",
    "            # z^{l+1} and a^{l+1}\n",
    "            z = self.zs[l]\n",
    "            a = self.activations[l+1]\n",
    "    \n",
    "            # derivative of activation\n",
    "            if self.activation_function == 'relu':\n",
    "                d_activation = (z > 0).astype(float)\n",
    "            elif self.activation_function == 'sigmoid':\n",
    "                d_activation = a * (1 - a)\n",
    "    \n",
    "            # dZ = dA * activation\n",
    "            dZ = dA * d_activation\n",
    "    \n",
    "            # compute gradients for this layer\n",
    "            dW[l] = np.dot(self.activations[l].T, dZ)\n",
    "            db[l] = np.sum(dZ, axis=0, keepdims=True)\n",
    "    \n",
    "            # compute next dA (unless we reach first layer)\n",
    "            if l > 0:\n",
    "                dA = np.dot(dZ, self.weights[l].T)\n",
    "    \n",
    "        # update all weights and biases\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * dW[i]\n",
    "            self.biases[i]  -= self.learning_rate * db[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148fa57-4d24-43ba-92b2-e5dcf126de93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d87b1-9269-4d84-8873-8c767fa6fa8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d422d6e1-a3b0-4dd2-a033-50003fd54bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(\n",
    "    activation_function='relu',\n",
    "    no_of_input_nodes=X_train.shape[1],       # input features\n",
    "    no_of_hidden_nodes=[8, 8],                # two hidden layers, each layer has 8 neurons\n",
    "    no_of_output_nodes=len(np.unique(y_train)),\n",
    "    n_epochs=1000,\n",
    "    activation_output='softmax',\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5a2cacd7-d1b7-41da-91d3-19effb1fc89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000, loss = 1.4626\n",
      "Epoch 100/1000, loss = 0.7816\n",
      "Epoch 150/1000, loss = 0.6246\n",
      "Epoch 200/1000, loss = 0.5436\n",
      "Epoch 250/1000, loss = 0.4913\n",
      "Epoch 300/1000, loss = 0.4536\n",
      "Epoch 350/1000, loss = 0.4243\n",
      "Epoch 400/1000, loss = 0.3999\n",
      "Epoch 450/1000, loss = 0.3804\n",
      "Epoch 500/1000, loss = 0.3634\n",
      "Epoch 550/1000, loss = 0.3486\n",
      "Epoch 600/1000, loss = 0.3355\n",
      "Epoch 650/1000, loss = 0.3237\n",
      "Epoch 700/1000, loss = 0.3130\n",
      "Epoch 750/1000, loss = 0.3030\n",
      "Epoch 800/1000, loss = 0.2937\n",
      "Epoch 850/1000, loss = 0.2852\n",
      "Epoch 900/1000, loss = 0.2775\n",
      "Epoch 950/1000, loss = 0.2703\n",
      "Epoch 1000/1000, loss = 0.2633\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nn.n_epochs):\n",
    "    # forward\n",
    "    y_pred = nn.forward_pass(X_train)\n",
    "\n",
    "    # loss\n",
    "    loss = nn.compute_loss(y_pred, y_train)\n",
    "\n",
    "    # backward\n",
    "    nn.backward(X_train, y_train, y_pred)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{nn.n_epochs}, loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "27c918c7-a434-4567-b973-5b254de9956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 97.50%\n",
      "Test accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "# train accuracy\n",
    "y_train_pred = np.argmax(nn.forward_pass(X_train), axis=1)\n",
    "train_acc = np.mean(y_train_pred == y_train)\n",
    "print(f\"Train accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "# test accuracy\n",
    "y_test_pred = np.argmax(nn.forward_pass(X_test), axis=1)\n",
    "test_acc = np.mean(y_test_pred == y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea0bdb-e4fc-4973-9c6b-d8e7470ba068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b38dca-901b-4b29-8ff8-ef63573d4f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f8c7a-1d6f-46e5-9d6b-8c0db74d4e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3faa5e-f3b6-4910-8c3e-ad782d4f22b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2adb0-8ddc-4158-85c7-c6ff66db4607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712dbbbb-4be2-44dc-bbe9-d6ef05c49b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d13a3-650f-4db6-93fc-cc7f5a423149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
