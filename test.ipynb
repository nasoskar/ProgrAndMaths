{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7f4f4a",
   "metadata": {},
   "source": [
    "## Programming and Maths for AI: Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afe7e5",
   "metadata": {},
   "source": [
    "#### Activation functions: sigmoid and ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459e72b",
   "metadata": {},
   "source": [
    "Sigmoid and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedd5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    s = 1/(1 + np.exp(-x))\n",
    "\n",
    "    return s\n",
    "\n",
    "def derivative_sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    ds = s*(1-s)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ef17c",
   "metadata": {},
   "source": [
    "ReLU and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c95356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return 1 * (x>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c803de9",
   "metadata": {},
   "source": [
    "Now let's implement the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24737ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def softmax(vector):\n",
    "#    e = np.exp(vector)\n",
    "#    return e / e.sum()\n",
    "\n",
    "# if z is very large, exp(z) can overflow\n",
    "# solution: subtract by max\n",
    "def softmax(z): #stable, so we dont have exploding issues due to exp\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120e733",
   "metadata": {},
   "source": [
    "### Implement a fully parametrizable neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45cc2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     species  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "145        2  \n",
       "146        2  \n",
       "147        2  \n",
       "148        2  \n",
       "149        2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['species'] = iris.target\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fcf7f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "65                 6.7               3.1                4.4               1.4\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "42                 4.4               3.2                1.3               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "71                 6.1               2.8                4.0               1.3\n",
       "106                4.9               2.5                4.5               1.7\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "92                 5.8               2.6                4.0               1.2\n",
       "102                7.1               3.0                5.9               2.1\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1a01f",
   "metadata": {},
   "source": [
    "## Create a NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1a8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_np = X_train.values if hasattr(X_train, 'values') else np.array(X_train)\n",
    "y_train_np = y_train.values if hasattr(y_train, 'values') else np.array(y_train)\n",
    "\n",
    "X_test_np = X_test.values if hasattr(X_test, 'values') else np.array(X_test)\n",
    "y_test_np = y_test.values if hasattr(y_test, 'values') else np.array(y_test)\n",
    "\n",
    "# just 4 samples\n",
    "X = np.array(X_train)\n",
    "\n",
    "# target values \n",
    "y = np.array(y_train).T \n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    #hidden layer (sigmoid, relu)\n",
    "    #output layer (softmax)\n",
    "\n",
    "    def __init__(self, activation_function, no_of_input_nodes, no_of_hidden_nodes, no_of_output_nodes, n_epochs,\n",
    "                 lambda1,lambda2,lr,dropout, optimizer, momentum):\n",
    "        self.hidden_layers = len(no_of_hidden_nodes)\n",
    "        self.activation_function = activation_function\n",
    "        self.no_of_input_nodes = no_of_input_nodes # as many as the dataset's features\n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes # no fixed number, needs tuning\n",
    "        self.no_of_output_nodes = no_of_output_nodes # as many as the output classes\n",
    "        self.n_epochs = n_epochs #no. of epochs to run the NN\n",
    "        self.lambda1 = lambda1 #lambda variable for L1 regularisation\n",
    "        self.lambda2 = lambda2 #lambda variable for L2 regularisation\n",
    "        self.learning_rate = lr #how much to update the weights\n",
    "        self.z_values = [] # need to store for backprop\n",
    "        self.a_values = []   # need to store for backprop, first value is the actual data\n",
    "        self.dropout_masks = [] # save the masks for backprop\n",
    "        self.dropout = dropout #probability for dropout\n",
    "        self.weights, self.biases = self.weights_and_bias()\n",
    "        \n",
    "        #optimizer settings\n",
    "        self.optimizer = optimizer # the optimizer that will be used\n",
    "        self.momentum = momentum\n",
    "        self.vW = [np.zeros_like(W) for W in self.weights]  # velocity for momentum\n",
    "        self.vb = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        \n",
    "\n",
    "    def weights_and_bias(self):\n",
    "        layers = [self.no_of_input_nodes] + self.no_of_hidden_nodes + [self.no_of_output_nodes] #e.g. [2,3,5,2]\n",
    "        weights = [] #TODO: weight and bias proper initialization (xavier)\n",
    "        biases = []\n",
    "        for i in range(len((layers))-1):\n",
    "\n",
    "            n_in = layers[i]\n",
    "            n_out = layers[i+1]\n",
    "\n",
    "            weights.append(2*np.random.random((n_in,n_out)) - 1)\n",
    "            biases.append(np.zeros((1, n_out)))\n",
    "        return weights, biases\n",
    "    \n",
    "    def inverted_dropout(self, x, p):\n",
    "        # p = dropout probability\n",
    "        mask = (np.random.rand(*x.shape) > p).astype(float)\n",
    "        self.dropout_masks.append(mask)\n",
    "        x_dropped = (x * mask)/(1 - p) # the actual dropout\n",
    "        \n",
    "        return x_dropped\n",
    "    \n",
    "    def forward_pass(self, X, training=True): #add training flag for dropout\n",
    "\n",
    "        # Clear previous batch values\n",
    "        self.a_values = []\n",
    "        self.z_values = []\n",
    "        self.dropout_masks = []\n",
    "\n",
    "        #first hidden layer needs to have the actual data\n",
    "        #all other hidden layers take the result from the previous layer\n",
    "        a = X\n",
    "        self.a_values.append(a) # save for backprop\n",
    "        \n",
    "        for layer in range(self.hidden_layers):\n",
    "\n",
    "            W = self.weights[layer]\n",
    "            b = self.biases[layer]\n",
    "            z = np.dot(a, W) + b\n",
    "            self.z_values.append(z) # save for backprop\n",
    "\n",
    "            if self.activation_function =='sigmoid':\n",
    "                a = sigmoid(z)\n",
    "            elif self.activation_function =='relu':\n",
    "                a = relu(z)\n",
    "            #print(f'Shape of layer: {a.shape}')\n",
    "\n",
    "            #implementing inverted dropout\n",
    "            if training: # In testing, dropout is not applied\n",
    "                a = self.inverted_dropout(a, self.dropout) \n",
    "\n",
    "            self.a_values.append(a) # save for backprop\n",
    "\n",
    "        #---Output Layer---\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        z = np.dot(a, W) + b\n",
    "        self.z_values.append(z) # save for backprop\n",
    "        a = softmax(z)\n",
    "        self.a_values.append(a) # save for backprop\n",
    "        #print(f'Shape of output layer: {a.shape}')\n",
    "        return a\n",
    "        \n",
    "\n",
    "    def backward_pass(self, X_train, y_true):\n",
    "        \n",
    "        N = y_true.shape[0]\n",
    "\n",
    "        # We convert labels to one-hot so that they match the shape of y_true\n",
    "        y_one_hot = np.zeros((N, self.no_of_output_nodes))\n",
    "        y_one_hot[np.arange(N), y_true] = 1\n",
    "\n",
    "        # make empty lists for gradients\n",
    "        dW = [None] * len(self.weights)\n",
    "        db = [None] * len(self.biases)\n",
    "\n",
    "        # --- Output layer ---\n",
    "        delta = self.a_values[-1] - y_one_hot  # Derivative of loss: softmax + CE derivative (a(L) - y)\n",
    "        \n",
    "        dW[-1] = self.a_values[-2].T.dot(delta) / N\n",
    "\n",
    "        dW[-1] += self.lambda1 * np.sign(self.weights[-1])   # L1 gradient\n",
    "        dW[-1] += 2 * self.lambda2 * self.weights[-1]       # L2 gradient\n",
    "\n",
    "\n",
    "        db[-1] = np.mean(delta, axis=0, keepdims=True) #gradient of bias\n",
    "\n",
    "        #self.weights[-1] -= self.learning_rate * dW[-1] # update the weight\n",
    "        #self.biases[-1] -= self.learning_rate * db[-1] # update the bias\n",
    "\n",
    "        # --- Hidden layers ---\n",
    "        for layer in range(self.hidden_layers - 1, -1, -1):\n",
    "\n",
    "            # Backprop error\n",
    "            delta = delta.dot(self.weights[layer + 1].T)\n",
    "\n",
    "            #use dropout\n",
    "            delta *= self.dropout_masks[layer]\n",
    "\n",
    "            if self.activation_function =='sigmoid':\n",
    "                delta *= derivative_sigmoid(self.z_values[layer])\n",
    "            elif self.activation_function =='relu':\n",
    "                delta *= derivative_relu(self.z_values[layer])\n",
    "            \n",
    "            a_prev = X_train if layer == 0 else self.a_values[layer]\n",
    "            dW[layer] = a_prev.T.dot(delta) / N \n",
    "            db[layer] = np.mean(delta, axis=0, keepdims=True) #gradient of bias\n",
    "\n",
    "            dW[layer] += self.lambda1 * np.sign(self.weights[layer])   # L1 gradient\n",
    "            dW[layer] += 2 * self.lambda2 * self.weights[layer]       # L2 gradient\n",
    "\n",
    "            \n",
    "            #self.weights[layer] -= self.learning_rate * dW[layer] # update the weight\n",
    "            #self.biases[layer] -= self.learning_rate * db[layer] # update the bias\n",
    "\n",
    "        self.choose_optimizer(dW, db)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        Cross entropy loss for multi-class classification with L1 and L2 regularization\n",
    "        '''\n",
    "\n",
    "        # number of samples\n",
    "        N = y_true.shape[0]\n",
    "        \n",
    "        correct_probs = y_pred[np.arange(N), y_true] #TODO: change to binary formula if dataset is binary\n",
    "    \n",
    "        # loss = average of -log(p) whre p is the predicted probability of the correct class\n",
    "        loss = -np.sum(np.log(correct_probs)) / N\n",
    "\n",
    "        l1_loss = self.lambda1 * sum(np.sum(np.abs(W)) for W in self.weights)\n",
    "        l2_loss = self.lambda2 * sum(np.sum(W**2) for W in self.weights)\n",
    "\n",
    "\n",
    "        return loss + l1_loss + l2_loss\n",
    "    \n",
    "    def choose_optimizer(self, dW, db): # function to call the respective optimizer method to be used\n",
    "        if self.optimizer == \"momentum\":\n",
    "            self.momentum_update(dW, db)\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.adam_update(dW, db)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "        \n",
    "    def momentum_update(self,dW,db):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.vW[i] = self.momentum * self.vW[i] + (1 - self.momentum) * dW[i]\n",
    "            self.vb[i] = self.momentum * self.vb[i] + (1 - self.momentum) * db[i]\n",
    "            self.weights[i] -= self.learning_rate * self.vW[i]\n",
    "            self.biases[i]  -= self.learning_rate * self.vb[i]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "186ee072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 543.1824, Accuracy: 0.3417\n",
      "Epoch 5, Loss: 538.6668, Accuracy: 0.3333\n",
      "Epoch 10, Loss: 530.6938, Accuracy: 0.3167\n",
      "Epoch 15, Loss: 520.5474, Accuracy: 0.2917\n",
      "Epoch 20, Loss: 508.2607, Accuracy: 0.3083\n",
      "Epoch 25, Loss: 495.0442, Accuracy: 0.3500\n",
      "Epoch 30, Loss: 482.4752, Accuracy: 0.3667\n",
      "Epoch 35, Loss: 469.5084, Accuracy: 0.3333\n",
      "Epoch 40, Loss: 456.9961, Accuracy: 0.3083\n",
      "Epoch 45, Loss: 444.4383, Accuracy: 0.3250\n",
      "Epoch 50, Loss: 432.0897, Accuracy: 0.2917\n",
      "Epoch 55, Loss: 420.0829, Accuracy: 0.3417\n",
      "Epoch 60, Loss: 408.4005, Accuracy: 0.3667\n",
      "Epoch 65, Loss: 397.0564, Accuracy: 0.3667\n",
      "Epoch 70, Loss: 385.8525, Accuracy: 0.3917\n",
      "Epoch 75, Loss: 375.2078, Accuracy: 0.3917\n",
      "Epoch 80, Loss: 364.7857, Accuracy: 0.3000\n",
      "Epoch 85, Loss: 354.2515, Accuracy: 0.2750\n",
      "Epoch 90, Loss: 344.0690, Accuracy: 0.3750\n",
      "Epoch 95, Loss: 333.9475, Accuracy: 0.3917\n"
     ]
    }
   ],
   "source": [
    "#Param tuning:\n",
    "#lambda1 and lambda 2 can be from 0 to 1\n",
    "\n",
    "X_small = X_train_np[:10]\n",
    "y_small = y_train_np[:10]\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    activation_function='sigmoid',\n",
    "    no_of_input_nodes=X_train.shape[1], # input features\n",
    "    no_of_hidden_nodes=[32,64,64],\n",
    "    no_of_output_nodes=3, #TODO len(np.unique(y_train)?\n",
    "    n_epochs=100,\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    lr=0.01,\n",
    "    dropout=0.5,\n",
    "    optimizer=\"momentum\",\n",
    "    momentum=0.9\n",
    "    )\n",
    "\n",
    "for epoch in range(nn.n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = nn.forward_pass(X_train_np, training=True)\n",
    "\n",
    "    # Loss function\n",
    "    loss = nn.compute_loss(y_pred, y_train_np)\n",
    "\n",
    "    # Backward pass\n",
    "    nn.backward_pass(X_train_np, y_train_np)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        accuracy = np.mean(y_pred_labels == y_train_np)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df0dec",
   "metadata": {},
   "source": [
    "| Parameter       | Description                                        | Where it is used                                                     |\n",
    "| --------------- | -------------------------------------------------- | -------------------------------------------------------------------- |\n",
    "| `learning_rate` | Step size for gradient descent                     | Used in weight update: `W -= learning_rate * grad_W`                 |\n",
    "| `update_rule`   | Optimization method (`sgd`, `momentum`, `adam`)    | Determines how gradients are applied to weights                      |\n",
    "| `decay`         | Learning rate decay per epoch                      | `learning_rate *= (1 / (1 + decay * epoch))`                         |\n",
    "| `epochs`        | Number of times to iterate over the entire dataset | Controls the main training loop                                      |\n",
    "| `batch_size`    | Number of samples per mini-batch                   | Used to split data into mini-batches for stochastic gradient descent |\n",
    "| `momentum`      | Momentum coefficient (if using momentum optimizer) | Used in update: `v = beta*v + (1-beta)*grad`                         |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
